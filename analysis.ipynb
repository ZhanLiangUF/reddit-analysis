{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.423  0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenization and sentiment analysis\n",
    "\"\"\"\n",
    "# conn.cursor().execute(\"\"\"DROP TABLE linkusersentimenttosubreddit\"\"\")\n",
    "sql_create_linkusersentimenttosubreddit_table = \"\"\" CREATE TABLE IF NOT EXISTS linkusersentimenttosubreddit (\n",
    "                                        userid text,\n",
    "                                        subredditid text,\n",
    "                                        rating integer\n",
    "                                    ); \"\"\"\n",
    "conn.cursor().execute(sql_create_linkusersentimenttosubreddit_table)\n",
    "\n",
    "\"\"\"\n",
    "The snippet below is ran onced - to insert data into a new table\n",
    "\"\"\"\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "# for comment in reddit_comments:\n",
    "#     array_of_sentences = nltk.sent_tokenize(comment[2])\n",
    "#     comment_sentiment = 0\n",
    "#     for sentence in array_of_sentences:\n",
    "#         ss = sid.polarity_scores(sentence)\n",
    "#         array_of_tokenized_values = []\n",
    "#         comment_sentiment += ss['pos']  \n",
    "#     conn.cursor().execute(\"INSERT INTO linkusersentimenttosubreddit VALUES (?, ?, ?)\", (comment[1], comment[0], comment_sentiment))\n",
    "#     conn.commit()\n",
    "newtable = conn.cursor().execute(\"SELECT * from linkusersentimenttosubreddit\").fetchall()\n",
    "comments_df = pd.DataFrame(newtable, columns = ['UserID', 'SubredditID', 'Rating'])\n",
    "R_df = comments_df.pivot(index = 'UserID', columns ='SubredditID', values = 'Rating').fillna(0)\n",
    "R_df.head()\n",
    "\n",
    "# # \"\"\"\n",
    "# # convert to a numpy array and normalize data\n",
    "# # \"\"\"\n",
    "R = R_df.as_matrix()\n",
    "print(R)\n",
    "ratings_mean = np.mean(R)\n",
    "ratings_mean.reshape(-1,1)\n",
    "R_demeaned = R - ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('t5_378oi', 't1_cqug90g', 'くそ\\n読みたいが買ったら負けな気がする\\n図書館に出ねーかな')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "conn = create_connection(\"database.sqlite\")\n",
    "    \n",
    "reddit_comments = conn.cursor().execute(\"SELECT subreddit_id, name, body  from May2015 LIMIT 5\").fetchall()\n",
    "reddit_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alright now to perform svd\n",
    "\"\"\"\n",
    "U, sigma, Vt = svds(R_demeaned, k = 2)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "all_users_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + ratings_mean.reshape(-1,1)\n",
    "preds_df = pd.DataFrame(all_users_predicted_ratings, columns = R_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alright lets make the recommendation engine\n",
    "\"\"\"\n",
    "def recommend_subreddits(predictions_df, userID, comments_df, original_ratings_df, num_recommendations=5):\n",
    "    #Get and sort user's predictions\n",
    "    user_row_number = userID - 1\n",
    "    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)\n",
    "    \n",
    "    #Get user's data and merge in the comments data\n",
    "    user_Data = N\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
